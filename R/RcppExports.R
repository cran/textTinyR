# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

Path_2vector <- function(path_2folder = "", path_2file = "") {
    .Call('textTinyR_Path_2vector', PACKAGE = 'textTinyR', path_2folder, path_2file)
}

Frequency_distribution <- function(x, path_2folder = "", path_2file = "", file_delimiter = '\n') {
    .Call('textTinyR_Frequency_distribution', PACKAGE = 'textTinyR', x, path_2folder, path_2file, file_delimiter)
}

Count_characters <- function(x, path_2folder = "", path_2file = "", file_delimiter = '\n') {
    .Call('textTinyR_Count_characters', PACKAGE = 'textTinyR', x, path_2folder, path_2file, file_delimiter)
}

Collocations_ngrams <- function(x, path_2folder = "", path_2file = "", file_delimiter = '\n', n_gram_delimiter = "_") {
    .Call('textTinyR_Collocations_ngrams', PACKAGE = 'textTinyR', x, path_2folder, path_2file, file_delimiter, n_gram_delimiter)
}

Dice_similarity <- function(x, y, n_grams) {
    .Call('textTinyR_Dice_similarity', PACKAGE = 'textTinyR', x, y, n_grams)
}

Levenshtein_dist <- function(s, t) {
    .Call('textTinyR_Levenshtein_dist', PACKAGE = 'textTinyR', s, t)
}

Cosine_dist <- function(x, y, split_separator = " ") {
    .Call('textTinyR_Cosine_dist', PACKAGE = 'textTinyR', x, y, split_separator)
}

Dissimilarity_mat <- function(words, dice_n_gram = 2L, method = "dice", split_separator = " ", dice_thresh = 0.3, upper = TRUE, diagonal = TRUE, threads = 1L) {
    .Call('textTinyR_Dissimilarity_mat', PACKAGE = 'textTinyR', words, dice_n_gram, method, split_separator, dice_thresh, upper, diagonal, threads)
}

Look_up_tbl <- function(VEC, n_grams) {
    .Call('textTinyR_Look_up_tbl', PACKAGE = 'textTinyR', VEC, n_grams)
}

res_token <- function(x, language, language_spec, LOCALE_UTF, FLAG_path, read_file_delimiter, max_num_char, remove_char = "", cpp_to_lower = FALSE, cpp_to_upper = FALSE, cpp_remove_punctuation = FALSE, remove_punctuation_vector = FALSE, cpp_remove_numbers = FALSE, cpp_trim_token = FALSE, cpp_tokenization_function = FALSE, cpp_string_separator = "-*", cpp_remove_stopwords = FALSE, min_num_char = 1L, stemmer = "NULL", min_n_gram = 1L, max_n_gram = 1L, skip_n_gram = 1L, skip_distance = 0L, n_gram_delimiter = " ", concat_delimiter = "NULL", path_2file = "", stemmer_ngram = 4L, stemmer_gamma = 0.0, stemmer_truncate = 3L, stemmer_batches = 1L, threads = 1L, verbose = FALSE, save_2single_file = FALSE, path_extend = "output_token.txt", vocabulary_path = "") {
    .Call('textTinyR_res_token', PACKAGE = 'textTinyR', x, language, language_spec, LOCALE_UTF, FLAG_path, read_file_delimiter, max_num_char, remove_char, cpp_to_lower, cpp_to_upper, cpp_remove_punctuation, remove_punctuation_vector, cpp_remove_numbers, cpp_trim_token, cpp_tokenization_function, cpp_string_separator, cpp_remove_stopwords, min_num_char, stemmer, min_n_gram, max_n_gram, skip_n_gram, skip_distance, n_gram_delimiter, concat_delimiter, path_2file, stemmer_ngram, stemmer_gamma, stemmer_truncate, stemmer_batches, threads, verbose, save_2single_file, path_extend, vocabulary_path)
}

res_token_vector <- function(VEC, language, language_spec, LOCALE_UTF, max_num_char, remove_char = "", cpp_to_lower = FALSE, cpp_to_upper = FALSE, cpp_remove_punctuation = FALSE, remove_punctuation_vector = FALSE, cpp_remove_numbers = FALSE, cpp_trim_token = FALSE, cpp_tokenization_function = FALSE, cpp_string_separator = "-*", cpp_remove_stopwords = FALSE, min_num_char = 1L, stemmer = "NULL", min_n_gram = 1L, max_n_gram = 1L, skip_n_gram = 1L, skip_distance = 0L, n_gram_delimiter = " ", concat_delimiter = "NULL", path_2file = "", stemmer_ngram = 4L, stemmer_gamma = 0.0, stemmer_truncate = 3L, stemmer_batches = 1L, threads = 1L, verbose = FALSE, vocabulary_path = "") {
    .Call('textTinyR_res_token_vector', PACKAGE = 'textTinyR', VEC, language, language_spec, LOCALE_UTF, max_num_char, remove_char, cpp_to_lower, cpp_to_upper, cpp_remove_punctuation, remove_punctuation_vector, cpp_remove_numbers, cpp_trim_token, cpp_tokenization_function, cpp_string_separator, cpp_remove_stopwords, min_num_char, stemmer, min_n_gram, max_n_gram, skip_n_gram, skip_distance, n_gram_delimiter, concat_delimiter, path_2file, stemmer_ngram, stemmer_gamma, stemmer_truncate, stemmer_batches, threads, verbose, vocabulary_path)
}

res_token_list <- function(VEC, language, language_spec, LOCALE_UTF, max_num_char, remove_char = "", cpp_to_lower = FALSE, cpp_to_upper = FALSE, cpp_remove_punctuation = FALSE, remove_punctuation_vector = FALSE, cpp_remove_numbers = FALSE, cpp_trim_token = FALSE, cpp_tokenization_function = FALSE, cpp_string_separator = "-*", cpp_remove_stopwords = FALSE, min_num_char = 1L, stemmer = "NULL", min_n_gram = 1L, max_n_gram = 1L, skip_n_gram = 1L, skip_distance = 0L, n_gram_delimiter = " ", concat_delimiter = "NULL", path_2file = "", stemmer_ngram = 4L, stemmer_gamma = 0.0, stemmer_truncate = 3L, stemmer_batches = 1L, threads = 1L, verbose = FALSE, vocabulary_path = "") {
    .Call('textTinyR_res_token_list', PACKAGE = 'textTinyR', VEC, language, language_spec, LOCALE_UTF, max_num_char, remove_char, cpp_to_lower, cpp_to_upper, cpp_remove_punctuation, remove_punctuation_vector, cpp_remove_numbers, cpp_trim_token, cpp_tokenization_function, cpp_string_separator, cpp_remove_stopwords, min_num_char, stemmer, min_n_gram, max_n_gram, skip_n_gram, skip_distance, n_gram_delimiter, concat_delimiter, path_2file, stemmer_ngram, stemmer_gamma, stemmer_truncate, stemmer_batches, threads, verbose, vocabulary_path)
}

big_splitter_bytes <- function(input_path, batches, end_query, OUTPUT_PATH, trimmed_line = FALSE, verbose = FALSE) {
    invisible(.Call('textTinyR_big_splitter_bytes', PACKAGE = 'textTinyR', input_path, batches, end_query, OUTPUT_PATH, trimmed_line, verbose))
}

big_parser <- function(input_path_folder, start_query, end_query, output_path_folder, min_lines = 1L, trimmed_line = FALSE, verbose = FALSE) {
    invisible(.Call('textTinyR_big_parser', PACKAGE = 'textTinyR', input_path_folder, start_query, end_query, output_path_folder, min_lines, trimmed_line, verbose))
}

file_parser <- function(input_path_file, start_query, end_query, output_path_file = "", min_lines = 1L, trimmed_line = FALSE, verbose = FALSE) {
    invisible(.Call('textTinyR_file_parser', PACKAGE = 'textTinyR', input_path_file, start_query, end_query, output_path_file, min_lines, trimmed_line, verbose))
}

convert_bytes <- function(input_path_file, unit = "GB") {
    .Call('textTinyR_convert_bytes', PACKAGE = 'textTinyR', input_path_file, unit)
}

big_tokenize <- function(input_path_folder, output_path_folder, batches, language, language_spec, LOCALE_UTF, read_file_delimiter, max_num_char, increment_batch_no = 1L, remove_char = "", cpp_to_lower = FALSE, cpp_to_upper = FALSE, cpp_remove_punctuation = FALSE, remove_punctuation_vector = FALSE, cpp_remove_numbers = FALSE, cpp_trim_token = FALSE, cpp_tokenization_function = FALSE, cpp_string_separator = "-*", cpp_remove_stopwords = FALSE, min_num_char = 1L, stemmer = "NULL", min_n_gram = 1L, max_n_gram = 1L, skip_n_gram = 1L, skip_distance = 0L, n_gram_delimiter = " ", concat_delimiter = "NULL", stemmer_ngram = 4L, stemmer_gamma = 0.0, stemmer_truncate = 3L, stemmer_batches = 1L, threads = 1L, save_2single_file = FALSE, vocabulary_folder = "", verbose = FALSE) {
    invisible(.Call('textTinyR_big_tokenize', PACKAGE = 'textTinyR', input_path_folder, output_path_folder, batches, language, language_spec, LOCALE_UTF, read_file_delimiter, max_num_char, increment_batch_no, remove_char, cpp_to_lower, cpp_to_upper, cpp_remove_punctuation, remove_punctuation_vector, cpp_remove_numbers, cpp_trim_token, cpp_tokenization_function, cpp_string_separator, cpp_remove_stopwords, min_num_char, stemmer, min_n_gram, max_n_gram, skip_n_gram, skip_distance, n_gram_delimiter, concat_delimiter, stemmer_ngram, stemmer_gamma, stemmer_truncate, stemmer_batches, threads, save_2single_file, vocabulary_folder, verbose))
}

vocabulary_counts_big_tokenize <- function(input_path_folder, output_path_file, max_num_chars = 1000L, verbose = FALSE) {
    invisible(.Call('textTinyR_vocabulary_counts_big_tokenize', PACKAGE = 'textTinyR', input_path_folder, output_path_file, max_num_chars, verbose))
}

vocabulary_counts <- function(input_path_file, start_query, end_query, language, output_path_file = "", min_lines = 1L, trimmed_line = FALSE, query_transform = FALSE, language_spec = "english", LOCALE_UTF = "", max_num_char = 1000000000L, remove_char = "", cpp_to_lower = FALSE, cpp_to_upper = FALSE, cpp_remove_punctuation = FALSE, remove_punctuation_vector = FALSE, cpp_remove_numbers = FALSE, cpp_trim_token = FALSE, cpp_tokenization_function = FALSE, cpp_string_separator = " \r\n\t.,;:()?!//", cpp_remove_stopwords = FALSE, min_num_char = 1L, stemmer = "NULL", min_n_gram = 1L, max_n_gram = 1L, skip_n_gram = 1L, skip_distance = 0L, n_gram_delimiter = " ", stemmer_ngram = 4L, stemmer_gamma = 0.0, stemmer_truncate = 3L, stemmer_batches = 1L, threads = 1L, verbose = FALSE) {
    invisible(.Call('textTinyR_vocabulary_counts', PACKAGE = 'textTinyR', input_path_file, start_query, end_query, language, output_path_file, min_lines, trimmed_line, query_transform, language_spec, LOCALE_UTF, max_num_char, remove_char, cpp_to_lower, cpp_to_upper, cpp_remove_punctuation, remove_punctuation_vector, cpp_remove_numbers, cpp_trim_token, cpp_tokenization_function, cpp_string_separator, cpp_remove_stopwords, min_num_char, stemmer, min_n_gram, max_n_gram, skip_n_gram, skip_distance, n_gram_delimiter, stemmer_ngram, stemmer_gamma, stemmer_truncate, stemmer_batches, threads, verbose))
}

batch_2file <- function(INPUT_FILE, OUTPUT_PATH, batches, read_file_delimiter, language, language_spec, LOCALE_UTF, max_num_char, remove_char = "", cpp_to_lower = FALSE, cpp_to_upper = FALSE, cpp_remove_punctuation = FALSE, remove_punctuation_vector = FALSE, cpp_remove_numbers = FALSE, cpp_trim_token = FALSE, cpp_tokenization_function = FALSE, cpp_string_separator = "-*", cpp_remove_stopwords = FALSE, min_num_char = 1L, stemmer = "NULL", min_n_gram = 1L, max_n_gram = 1L, skip_n_gram = 1L, skip_distance = 0L, n_gram_delimiter = " ", stemmer_ngram = 4L, stemmer_gamma = 0.0, stemmer_truncate = 3L, stemmer_batches = 1L, threads = 1L, concat_delimiter = "\n", vocabulary_path = "", verbose = FALSE) {
    invisible(.Call('textTinyR_batch_2file', PACKAGE = 'textTinyR', INPUT_FILE, OUTPUT_PATH, batches, read_file_delimiter, language, language_spec, LOCALE_UTF, max_num_char, remove_char, cpp_to_lower, cpp_to_upper, cpp_remove_punctuation, remove_punctuation_vector, cpp_remove_numbers, cpp_trim_token, cpp_tokenization_function, cpp_string_separator, cpp_remove_stopwords, min_num_char, stemmer, min_n_gram, max_n_gram, skip_n_gram, skip_distance, n_gram_delimiter, stemmer_ngram, stemmer_gamma, stemmer_truncate, stemmer_batches, threads, concat_delimiter, vocabulary_path, verbose))
}

res_term_matrix <- function(vector_corpus, language, language_spec, LOCALE_UTF, max_num_char, document_term_matrix = TRUE, path_2documents_file = "NULL", sort_columns = FALSE, remove_char = "", cpp_to_lower = FALSE, cpp_to_upper = FALSE, cpp_remove_punctuation = FALSE, remove_punctuation_vector = FALSE, cpp_remove_numbers = FALSE, cpp_trim_token = FALSE, cpp_tokenization_function = FALSE, cpp_string_separator = "-*", cpp_remove_stopwords = FALSE, min_num_char = 1L, stemmer = "NULL", min_n_gram = 1L, max_n_gram = 1L, skip_n_gram = 1L, skip_distance = 0L, n_gram_delimiter = " ", stemmer_ngram = 4L, stemmer_gamma = 0.0, stemmer_truncate = 3L, stemmer_batches = 1L, threads = 1L, verbose = FALSE, print_every_rows = 1000L, normalize_tf = "NULL", tf_idf = FALSE) {
    .Call('textTinyR_res_term_matrix', PACKAGE = 'textTinyR', vector_corpus, language, language_spec, LOCALE_UTF, max_num_char, document_term_matrix, path_2documents_file, sort_columns, remove_char, cpp_to_lower, cpp_to_upper, cpp_remove_punctuation, remove_punctuation_vector, cpp_remove_numbers, cpp_trim_token, cpp_tokenization_function, cpp_string_separator, cpp_remove_stopwords, min_num_char, stemmer, min_n_gram, max_n_gram, skip_n_gram, skip_distance, n_gram_delimiter, stemmer_ngram, stemmer_gamma, stemmer_truncate, stemmer_batches, threads, verbose, print_every_rows, normalize_tf, tf_idf)
}

Adj_Sparsity <- function(column_indices, row_indices, docs_counts, Terms, sparsity_thresh = 1.0) {
    .Call('textTinyR_Adj_Sparsity', PACKAGE = 'textTinyR', column_indices, row_indices, docs_counts, Terms, sparsity_thresh)
}

Associations_Cpp <- function(column_indices_, row_indices_, docs_counts_, target_size, Terms, mult_target_var, keepTerms = 0L, target_var = -1L, normalize_TF = "NULL", tf_IDF = FALSE, threads = 1L, verbose = FALSE) {
    .Call('textTinyR_Associations_Cpp', PACKAGE = 'textTinyR', column_indices_, row_indices_, docs_counts_, target_size, Terms, mult_target_var, keepTerms, target_var, normalize_TF, tf_IDF, threads, verbose)
}

Most_Freq_Terms <- function(sparse_data, Terms, keepTerms = 0L, flag_dtm = FALSE, threads = 1L, verbose = FALSE) {
    .Call('textTinyR_Most_Freq_Terms', PACKAGE = 'textTinyR', sparse_data, Terms, keepTerms, flag_dtm, threads, verbose)
}

sparsity_float <- function(data) {
    invisible(.Call('textTinyR_sparsity_float', PACKAGE = 'textTinyR', data))
}

dense_2sparse_mat <- function(x) {
    .Call('textTinyR_dense_2sparse_mat', PACKAGE = 'textTinyR', x)
}

sp_sums <- function(sp_data, rowSums = FALSE) {
    .Call('textTinyR_sp_sums', PACKAGE = 'textTinyR', sp_data, rowSums)
}

tf_idf_exclude <- function(tmp_mat, document_term_matrix = TRUE) {
    .Call('textTinyR_tf_idf_exclude', PACKAGE = 'textTinyR', tmp_mat, document_term_matrix)
}

sp_means <- function(sp_data, rowMeans = FALSE) {
    .Call('textTinyR_sp_means', PACKAGE = 'textTinyR', sp_data, rowMeans)
}

save_sparse_ <- function(x, file_name = "save_sparse.mat") {
    invisible(.Call('textTinyR_save_sparse_', PACKAGE = 'textTinyR', x, file_name))
}

load_sparse_ <- function(file_name = "load_sparse.mat") {
    .Call('textTinyR_load_sparse_', PACKAGE = 'textTinyR', file_name)
}

read_CHARS <- function(input_file, characters = 200L, write_2file = "") {
    .Call('textTinyR_read_CHARS', PACKAGE = 'textTinyR', input_file, characters, write_2file)
}

read_ROWS <- function(input_file, write_2file = "", read_delimiter = ' ', rows = 200L) {
    .Call('textTinyR_read_ROWS', PACKAGE = 'textTinyR', input_file, write_2file, read_delimiter, rows)
}

